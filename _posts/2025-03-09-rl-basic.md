---
title: RL의 기본 용어 및 분류
description: >-
  Get started with Chirpy basics in this comprehensive overview.
  You will learn how to install, configure, and use your first Chirpy-based website, as well as deploy it to a web server.
author: kenjee
categories: [AI, ML, RL]
tags: [Deepseek]
---

## 들어가며

인공지능(AI)이 마치 게임을 하듯 스스로 배우고 성장한다면 어떨까요?  
강화 학습(RL)은 AI가 주변 환경과 상호작용하면서 더 똑똑해지는 방법입니다.  
이번 글에서는 RL의 기본적인 용어와 다양한 분류 방법을 가볍게 살펴보겠습니다. 😊

--- 

## 1. 강화 학습(RL)이란?

강화 학습은 **목표를 달성하기 위해 시행착오를 거치며 최적의 행동을 학습하는 과정**입니다.  
지도 학습(Supervised Learning)과 달리, RL에서는 정답이 주어지지 않고, 환경과 상호작용하며 보상을 통해 학습합니다.

예를 들어, 체스 AI는 수많은 게임을 시뮬레이션하며 보상을 최대화하는 최적의 전략을 찾아갑니다.  
자율주행 자동차 역시 다양한 도로 상황을 경험하며 더 나은 주행 방식을 학습합니다.

| 머신 러닝 유형 | 학습 방식 | 예제 |
|--------------|----------|------|
| **지도 학습 (Supervised Learning)** | 정답이 주어진 데이터를 학습 | 이미지 분류, 스팸 필터링 |
| **비지도 학습 (Unsupervised Learning)** | 정답 없이 패턴을 찾음 | 군집 분석, 차원 축소 |
| **강화 학습 (Reinforcement Learning)** | 환경과 상호작용하며 최적의 행동 학습 | 게임 AI, 로봇 제어 |

---

## 2. 강화 학습의 핵심 개념

### 2.1 상태(State, \\( S \\))

강화 학습에서 상태는 **환경의 현재 상황**을 나타냅니다. 즉, 에이전트가 환경에서 경험하는 정보를 의미합니다.  
상태는 히스토리의 함수로 정의됩니다.

\\[ S_t = f(H_t) \\]

여기서 \\( H_t \\) 는 현재까지의 모든 관찰, 행동, 보상의 시퀀스를 포함하는 히스토리입니다.

---

### 2.2 행동(Action, \\( A \\))

에이전트가 환경에서 수행할 수 있는 선택입니다.  
각 상태 \\( S_t \\) 에 대해 가능한 행동 집합 \\( A_t \\) 가 주어집니다.

예제:  
- 체스 게임에서는 "말을 앞으로 이동"하는 것이 행동이 될 수 있습니다.  
- 자율주행 자동차에서는 "왼쪽으로 회전"하는 것이 행동이 됩니다.

---

### 2.3 보상(Reward, \\( R \\))

에이전트가 특정 행동을 수행한 후 환경에서 받는 신호입니다.  
강화 학습의 목표는 **총 보상(return)을 최대화하는 것**입니다.

즉각적인 보상은 다음과 같이 정의됩니다:

\\[ R_t = r(S_t, A_t) \\]

강화 학습은 단기적인 보상이 아닌 **장기적인 누적 보상(sum of rewards)** 을 최적화하는 것이 핵심입니다.

\\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\]

여기서 \\( \\gamma \\) (감가율, Discount Factor)는 미래 보상의 가중치를 조절하는 역할을 합니다.

---

### 2.4 정책(Policy, \\( \\pi \\))

정책은 **에이전트가 상태에서 어떤 행동을 선택할지 결정하는 전략**입니다.  
정책은 확률적으로 표현될 수도 있습니다:

\\[ \\pi(a | s) = P(A_t = a | S_t = s) \\]

즉, 특정 상태 \\( s \\) 에서 행동 \\( a \\) 를 선택할 확률을 나타냅니다.

---

### 2.5 가치 함수(Value Function, \\( V \\))

가치 함수는 특정 상태에서 기대할 수 있는 **장기적인 보상의 총합**을 예측하는 함수입니다.  
이는 정책을 평가하는 핵심 도구입니다.

상태 가치 함수:

\\[ V_\\pi(s) = E_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s \\right] \\]

행동 가치 함수:

\\[ Q_\\pi(s, a) = E_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid S_0 = s, A_0 = a \\right] \\]

---

추가 작성