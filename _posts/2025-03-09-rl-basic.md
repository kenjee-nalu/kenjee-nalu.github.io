---
title: RL의 기본 용어 및 분류
description: >-
  이번 포스팅에서는 강화 학습(RL)의 기본 개념과 다양한 분류 방법을 살펴봅니다.
math: true
categories: [ML, RL]
tags: [Deepseek]
---

### 들어가며
인공지능(AI)이 마치 게임을 하듯 스스로 배우고 성장한다면 어떨까요?  
강화 학습(RL)은 AI가 주변 환경과 상호작용하면서 더 똑똑해지는 방법입니다.
David Silver님의 유튜브 강의[^fn-01]를 참고하여 정리하였습니다.
이번 글에서는 RL의 기본적인 용어와 다양한 분류 방법을 가볍게 살펴보겠습니다. 😊

## 1. 강화 학습(RL)이란?

강화 학습은 **목표를 달성하기 위해 시행착오를 거치며 최적의 행동을 학습하는 과정**입니다.  
지도 학습(Supervised Learning)과 달리, RL에서는 정답이 주어지지 않고, 환경과 상호작용하며 보상을 통해 학습합니다.

예를 들어, 체스 AI는 수많은 게임을 시뮬레이션하며 보상을 최대화하는 최적의 전략을 찾아갑니다.  
자율주행 자동차 역시 다양한 도로 상황을 경험하며 더 나은 주행 방식을 학습합니다.

| 머신 러닝 유형 | 학습 방식 | 예제 |
|--------------|----------|------|
| **지도 학습 (Supervised Learning)** | 정답이 주어진 데이터를 학습 | 이미지 분류, 스팸 필터링 |
| **비지도 학습 (Unsupervised Learning)** | 정답 없이 패턴을 찾음 | 군집 분석, 차원 축소 |
| **강화 학습 (Reinforcement Learning)** | 환경과 상호작용하며 최적의 행동 학습 | 게임 AI, 로봇 제어 |

---

## 2. 강화 학습의 핵심 개념

### 2.1 상태(State, $S$)

강화 학습에서 상태는 **다음에 일어날 일을 결정하는 데 사용되는 정보**를 의미합니다. 상태는 히스토리의 함수로 정의됩니다:

$$S_t = f(H_t)$$

여기서 $H_t$는 현재까지의 모든 관찰, 행동, 보상의 시퀀스를 포함하는 히스토리입니다.

상태는 크게 다음 세 가지로 구분할 수 있습니다:

1. **환경 상태(Environment State)** $S^e_t$
   - 환경이 내부적으로 가지고 있는 표현
   - 환경이 다음 관찰과 보상을 결정하는 데 사용하는 정보
   - 일반적으로 에이전트에게 직접 보이지 않음
   - 보이더라도 불필요한 정보를 포함할 수 있음

2. **에이전트 상태(Agent State)** $S^a_t$
   - 에이전트의 내부 표현
   - 에이전트가 다음 행동을 선택하는 데 사용하는 정보
   - 강화 학습 알고리즘이 활용하는 정보
   - 히스토리의 임의의 함수가 될 수 있음: $S^a_t = f(H_t)$

3. **정보 상태(Information State)** 또는 **마르코프 상태(Markov State)**
   - 히스토리에서 유용한 모든 정보를 포함
   - 마르코프 속성을 만족: $P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t]$
   - "현재가 주어졌을 때, 미래는 과거와 독립적"이라는 의미
   - 상태를 알면 히스토리는 버려도 됨
   - 환경 상태 $S^e_t$와, 히스토리 $H_t$는 모두 마르코프 상태임

마르코프 속성은 강화 학습에서 중요한 개념으로, 현재 상태만 알면 미래 상태의 확률 분포를 결정할 수 있음을 의미합니다.

예를 들면 바둑에서:
- 현재 바둑판에 돌이 놓여있는 상태(현재 배치)만 알면 다음 가능한 상태들을 결정할 수 있습니다.
- 어떤 순서로 돌이 놓여졌는지(과거 히스토리)는 중요하지 않습니다.
- 오직 현재 바둑판의 상태만이 다음 수를 결정하는 데 필요한 모든 정보를 담고 있음을 의미합니다.

---

### 2.2 행동(Action, $A$)

에이전트가 환경에서 수행할 수 있는 선택입니다.  
각 상태 $S_t$ 에 대해 가능한 행동 집합 $A(S_t)$ 가 주어집니다.

예제:  
- 체스 게임에서는 "말을 앞으로 이동"하는 것이 행동이 될 수 있습니다.  
- 자율주행 자동차에서는 "왼쪽으로 회전"하는 것이 행동이 됩니다.

---

### 2.3 보상(Reward, $R$)

보상 $R_t$는 에이전트가 특정 행동을 수행한 후 환경으로부터 받는 스칼라 피드백 시그널입니다.  
이는 에이전트가 해당 단계에서 얼마나 잘 수행하고 있는지를 나타냅니다.

**에이전트의 목표는 누적 보상을 최대화하는 것입니다.**

강화 학습은 '보상 가설(Reward Hypothesis)'에 기반합니다:
> 모든 목표는 기대 누적 보상의 최대화로 설명될 수 있다.

#### 순차적 의사결정(Sequential Decision Making)

에이전트의 행동은 단기적인 결과뿐만 아니라 장기적인 결과도 가져올 수 있습니다:
- 보상이 지연될 수 있음
- 즉각적인 보상을 희생하여 더 큰 장기적 보상을 얻는 것이 더 나을 수 있음

강화 학습은
**장기적인 누적 보상(return)** 을 최적화하는 것이 핵심입니다:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

여기서 $\gamma$ (감가율, Discount Factor)는 미래 보상의 중요도를 조절하는 역할을 합니다 $(0 \leq \gamma \leq 1)$.

---

### 2.4 정책(Policy, $\pi$)

정책은 에이전트의 행동 방식을 정의하는 함수입니다. 즉, 정책은 각 상태에서 에이전트가 어떤 행동을 선택할지 결정합니다.

정책은 두 가지 형태로 표현될 수 있습니다:

1. **결정적 정책(Deterministic Policy)**: 각 상태에서 하나의 행동만을 선택합니다.
   $$a = \pi(s)$$

2. **확률적 정책(Stochastic Policy)**: 각 상태에서 가능한 행동들에 대한 확률 분포를 제공합니다.
   $$\pi(a|s) = P[A_t = a|S_t = s]$$

예를 들어, 바둑에서 정책은 현재 바둑판 상태에서 어느 위치에 돌을 놓을지 결정하는 전략입니다.
결정적 정책의 경우 특정 상태에서 항상 같은 수를 두지만, 확률적 정책에서는 확률에 따라 수를 두게 됩니다.

---

### 2.5 가치 함수(Value Function, $V$)

가치 함수는 미래 보상의 예측값으로, 특정 상태나 행동이 얼마나 "좋은지"를 평가합니다. 이를 통해 에이전트는 행동들 사이에서 선택할 수 있습니다.

**상태 가치 함수(State Value Function)** $v_\pi(s)$는 정책 $\pi$를 따를 때 상태 $s$에서 시작하여 얻을 수 있는 기대 누적 보상을 나타냅니다:

$$v_\pi(s) = E_\pi\left[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s\right]$$

**행동 가치 함수(Action Value Function)** $q_\pi(s,a)$는 상태 $s$에서 행동 $a$를 취하고 이후 정책 $\pi$를 따를 때 얻을 수 있는 기대 누적 보상을 나타냅니다.

가치 함수는 강화 학습에서 중요한 역할을 하며, 에이전트가 장기적으로 더 나은 결과를 가져오는 행동을 선택하도록 도와줍니다.

---

### 2.6 모델(Model)

모델은 환경의 동작을 예측하는 에이전트의 표현입니다. 모델은 다음과 같은 두 가지 주요 함수로 구성됩니다:

1. **상태 전이 함수(State Transition Function)** $P$: 다음 상태를 예측합니다.
   $$P^a_{ss'} = P[S_{t+1} = s' | S_t = s, A_t = a]$$
   
   이는 현재 상태 $s$에서 행동 $a$를 취했을 때 다음 상태가 $s'$가 될 확률을 나타냅니다.

2. **보상 함수(Reward Function)** $R$: 다음(즉각적인) 보상을 예측합니다.
   $$R^a_s = E[R_{t+1} | S_t = s, A_t = a]$$
   
   이는 현재 상태 $s$에서 행동 $a$를 취했을 때 받을 것으로 예상되는 즉각적인 보상을 나타냅니다.

모델은 에이전트가 계획을 세우고 미래 상황을 시뮬레이션할 수 있게 해주며, 특히 모델 기반(model-based) 강화 학습 알고리즘에서 중요한 역할을 합니다.

모델을 사용하지 않는 학습 방법을 모델 프리(model-free) 접근법이라고 하며, 이 경우 에이전트는 환경과의 직접적인 상호작용을 통해 정책이나 가치 함수를 학습합니다.
---

## 3. 강화 학습의 주요 문제

### 3.1 예측(Prediction) vs. 제어(Control)

- **예측(Prediction):** 주어진 정책을 따를 때 각 상태에서 기대할 수 있는 보상을 평가하는 과정입니다.  
- **제어(Control):** 최적의 정책을 학습하여 보상을 극대화하는 과정입니다.

### 3.2 탐색(Exploration) vs. 활용(Exploitation)

| 전략 | 장점 | 단점 |
|------|------|------|
| **탐색 (Exploration)** | 환경에 대한 정보를 더 많이 수집할 수 있음 | 보상이 낮을 가능성이 있음 |
| **활용 (Exploitation)** | 현재까지 학습한 최선의 전략을 사용하여 보상을 극대화 | 새로운 기회를 놓칠 수 있음 |

---

## 4. 강화 학습 에이전트의 분류
### 4.1 정책 사용 방식에 따른 분류

강화 학습 에이전트는 정책을 어떻게 학습하는지에 따라 크게 세 가지 유형으로 나뉩니다.

#### **가치 기반(Value-Based)**
- 정책을 명시적으로 학습하지 않고, **가치 함수(Value Function)를 기반으로 최적의 행동을 선택**하는 방식입니다.
- 행동 가치 함수 $Q(s, a)$ 를 학습하여, 각 상태에서 최대 가치(보상)를 제공하는 행동을 선택합니다.
- **예제 알고리즘:** Q-learning, Deep Q-Network (DQN)

#### **정책 기반(Policy-Based)**
- 가치 함수를 사용하지 않고, **정책(Policy) 자체를 직접 학습**하는 방식입니다.
- 확률적 정책을 사용하여 탐색을 수행하며, 연속적인 행동 공간에서 더 적합합니다.
- **예제 알고리즘:** REINFORCE, Proximal Policy Optimization (PPO)

#### **액터-크리틱(Actor-Critic)**
- **가치 기반과 정책 기반을 결합한 방식**으로, 정책과 가치 함수를 모두 학습합니다.
- **Actor(액터)**: 정책을 업데이트하여 행동을 결정합니다.
- **Critic(크리틱)**: 가치 함수를 통해 Actor의 행동을 평가합니다.
- **예제 알고리즘:** Advantage Actor-Critic (A2C), PPO, Deep Deterministic Policy Gradient (DDPG)
---

### 4.2 모델 사용 여부에 따른 분류

#### **모델 프리(Model-Free) 에이전트**
- **환경 모델 없이** 시행착오(Trial-and-Error)를 통해 직접 학습하는 방식입니다.
- **예제 알고리즘:** Q-learning, DQN, PPO

#### **모델 기반(Model-Based) 에이전트**
- **환경 모델을 학습하거나 활용하여 미래 상태와 보상을 예측**하는 방식입니다.
- **예제 알고리즘:** AlphaGo, MuZero, Dyna-Q

---

### 4.3 학습 방식에 따른 분류

#### **온-폴리시(On-Policy) 학습**
- 현재 사용 중인 정책을 지속적으로 업데이트하며 학습하는 방식입니다.
- **예제 알고리즘:** SARSA, PPO, A2C

#### **오프-폴리시(Off-Policy) 학습**
- 과거 데이터를 재사용하며 학습하는 방식으로, 다른 정책에서 생성된 데이터를 활용할 수도 있습니다.
- **예제 알고리즘:** Q-learning, DQN, DDPG

---

## 5. LLM에서의 RL 활용

최근 DeepSeek-R1의 열풍이 대단했는데요, 논문을 살펴보면 강화학습이 핵심적인 역할을 했다는 것을 알 수 있습니다. 아래에서 대규모 언어 모델(LLM)에서 강화학습이 어떻게 활용되고 있는지 살펴보겠습니다.

### 5.1 LLM 시대의 새로운 패러다임: 강화학습

초기 LLM 모델들은 주로 지도학습 방식으로 학습되었지만, 인간의 복잡한 의도와 가치를 모델에 반영하기 위해 강화학습(RL)이 도입되었습니다. ChatGPT가  RLHF(Reinforcement Learning from Human Feedback)를 적용한 이후, 이 방법론은 현대 LLM의 표준이 되었습니다.

RLHF의 일반적인 과정은 다음과 같습니다:
1. 기본 LLM 사전 훈련
2. 인간 평가자로부터 선호도 데이터 수집
3. 보상 모델 훈련
4. RL 기반 미세 조정(PPO 등 사용)

그러나 최근에는 인간 피드백의 한계(비용, 시간, 일관성)를 극복하기 위해 다양한 대안들이 등장했습니다:
- **Constitutional AI**: 사전 정의된 원칙에 따라 모델이 스스로 개선
- **RLAIF**: 다른 AI 모델의 피드백을 활용한 강화학습

### 5.2 DeepSeek-R1: 순수 RL로 추론 능력 향상하기

DeepSeek이 최근 공개한 DeepSeek-R1 모델은 강화학습을 활용한 접근법을 보여주었습니다. 특히 주목할 점은 다음과 같습니다:

#### DeepSeek-R1-Zero: 지도학습 없는 순수 RL 적용

DeepSeek-R1-Zero는 **지도 미세 조정(SFT) 없이** 기본 모델에 직접 대규모 강화학습을 적용한 모델입니다. 추론 능력이 지도학습 없이 RL만으로 자연스럽게 발현될 수 있음을 시사하였습니다.

DeepSeek-R1-Zero는 학습 과정에서 장문의 사고 과정(Chain-of-Thought) 생성한다던지, 자기 검증 및 여러 접근법을 시도 하는 등 독특한 추론 행동이 관찰되었습니다.

#### 다단계 훈련 파이프라인: DeepSeek-R1

DeepSeek-R1-Zero가 보여준 일부 한계(가독성 문제, 언어 혼합 등)를 해결하기 위해, DeepSeek은 아래와 같은 다단계 훈련 파이프라인을 도입했습니다:

1. **Cold-start 데이터 수집**: 소량의 초기 데이터로 기본 모델 미세 조정
2. **제1차 RL**: DeepSeek-R1-Zero와 유사한 추론 중심 강화학습 적용
3. **데이터 보강 및 SFT**: RL 체크포인트에서 생성된 데이터와 기존 지도 데이터 결합
4. **제2차 RL**: 모든 시나리오의 프롬프트를 고려한 추가 강화학습

이 과정을 통해 개발된 DeepSeek-R1은 OpenAI의 o1-1217과 유사한 수준의 성능을 달성했습니다.

### 5.3 DeepSeek-R1의 증류(Distillation) 전략

DeepSeek은 큰 모델에서 발견된 추론 패턴을 작은 모델로 증류하는 방법론도 제시했습니다. 이는 작은 모델에 직접 RL을 적용하는 것보다 더 효과적인 것으로 나타났습니다.

### 5.4 LLM에서 RL의 향후 방향

DeepSeek의 연구 결과는 LLM 개발에서 강화학습의 중요성을 재확인하고, 몇가지 중요한 의의를 남겼습니다.

1. **순수 RL의 가능성**: 지도학습 없이도 강화학습만으로 모델의 추론 능력을 크게 향상시킬 수 있습니다.

2. **효율적인 보상 설계**: 신경망 기반 보상 모델 대신 규칙 기반 보상 시스템을 활용했는데, 이는 보상 해킹(reward hacking) 문제를 피하면서도 효과적인 학습을 가능하게 했습니다.

3. **다단계 훈련 파이프라인**: RL과 SFT를 적절히 조합한 다단계 훈련이 최적의 결과를 가져올 수 있습니다.

4. **증류의 효과성**: 큰 모델의 추론 패턴은 작은 모델로 효과적으로 증류될 수 있으며, 이는 자원 제약이 있는 환경에서 고성능 모델을 배포하는 데 의의가 있습니다.

deepseek의 사례는 LLM 개발에서 강화학습이 단순히 선호도 반영을 넘어, 모델의 core capability 향상에도 핵심적인 역할을 할 수 있음을 보여줍니다. 앞으로도 더 효율적이고 효과적인 RL 방법론이 LLM의 발전을 이끌어갈 것으로 기대됩니다.
---

### Reference
[^fn-01]: [RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning](https://youtu.be/2pWv7GOvuf0?si=l6zCrtYaz0Y6t3ER)
[^fn-02]: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)
